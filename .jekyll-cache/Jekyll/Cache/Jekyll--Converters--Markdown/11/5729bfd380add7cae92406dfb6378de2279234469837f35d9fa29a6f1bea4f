I"w<hr />

<h3 id="introduction">Introduction</h3>

<hr />

<p>비교적 최근에 크리스토퍼 놀란의 신작 영화인 테넷을 보았다. 그런데 여기서 어딘가 정감가는 용어가 눈에 들어왔는데 바로 <strong>엔트로피</strong>였다. 엔트로피 어디서 들어본 것 같았는데… 바로 매번 모델에서 loss 함수에 사용했었던 교차 엔트로피랑 비슷해서 였다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># model.compile 에서 지긋지긋하게 쓰는 녀석들
</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="s">'Adam'</span><span class="p">,</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s">'categorical_crossentropy'</span><span class="p">,</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="p">)</span></code></pre></figure>

<p>그래서 인지 문득 궁금해졌다. 내가 항상 사용하는 loss 함수에 대한 개념을 잘 알고 있는가? 음.. 아닌 것 같다,, 그래서 이번 포스팅은 엔트로피란 무엇이고 교차 엔트로피란 또 무엇인지 그리고 가능하다면 교차 엔트로피에 대한 종류들도 다뤄 보려고 한다.</p>

<hr />

<h3 id="entropy-cross-entropy">Entropy, Cross Entropy</h3>

<hr />

<p>엔트로피(Entropy)란 학문의 관점에 따라 조금씩 다른 의미를 가지는데, 정보이론에서의 엔트로피를 이야기 해보자. 정보이론에서 높은 확률은 정보량이 작음을 의미한다(정보량은 사건의 확률과 관련이 있다). 예를 들면 한 여름에 맑을 확률은 높다 하지만 갑자기 눈이 내릴 확률은 거의 희박할 것이다. 그렇다면 확률과 정보량은 반비례하니 <strong>맑음 = 정보량, 작음 눈 = 정보량 큼</strong> 이렇게 분류 가능할 것이다. 다른 말로 놀람의 정도라고도 표현할 수 있다(한 여름에 눈이 왔으니 놀랄만 하지 않은가).
<br />
<br /></p>

<p>정보량을 나타내는 함수를 <strong>h</strong>라고 할 때,</p>

<ul>
  <li>확률 변수 <strong>X</strong> 가 존재하고 X는 sunny(맑음), Snow(눈)이라는 두 가지 값을 가질 수 있다.</li>
  <li><strong>X의 정보량 h(x)는 p(x)에 대한 함수 즉 h = f(p)이다.</strong> (p는 x가 벌어질 확률)</li>
  <li><strong>p(sunny) = 0.85, p(snow) = 0.15</strong> (물론 눈이 올 확률은 0.15보다 훨 낮겠지만..)</li>
  <li>
    <p><strong>h(snow) &gt; h(sunny)</strong> 이어야 한다. 정보량과 확률은 반비례하니까 당연하다.</p>

    <p>결론적으로 정보량에 대한 식이 아래와 같이 도출된다.</p>
  </li>
</ul>

\[\ h(x) = -log_{2}p(x)\]

<p>그렇다면 평균적인 정보량은</p>

\[한 여름에 눈이 오는 정보량 x 눈이 올 확률 + 한 여름에 맑은 정보량 x 맑을  확률이다.\]

<p>그리하여 아래와 같은 수식이 완성된다.</p>

\[\ H(X) = -\sum\limits_{i=1}^np(x_i)logp(x_i)\]

<p>엔트로피는 불확실성의 척도이다.</p>

<p>

</p>

<p><img src="https://user-images.githubusercontent.com/52132160/99909503-3d964000-2d2c-11eb-9c79-b261c7f2fdb9.png" alt="image" /></p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="syntax"><code>    source: https://www.cs.toronto.edu/~kriz/cifar.html
</code></pre></div></div>

<hr />

<h3 id="데이터-전처리">데이터 전처리</h3>

<p>
전처리에 앞서 필자는 colab을 사용한다. 노트북 밖에 없고 데스크탑 사양이 좋지 않은 필자 같은 분들은 colab을 사용하는 것을 권장한다(ipynb 환경이라 다운도 편하고 노트북 용량도 차지하지 않으니 좋다)
<br />
<br />
일단 코드부터 하나씩 천천히 진행하도록 하겠다.
</p>

<hr />

<p><br /></p>

<p>이 글은 팡요랩의 <a href="https://youtu.be/Dc0PQlNQhGY">KL Divergence 강의 영상</a> 을 참고하여 작성하였습니다.</p>

\[\]
:ET