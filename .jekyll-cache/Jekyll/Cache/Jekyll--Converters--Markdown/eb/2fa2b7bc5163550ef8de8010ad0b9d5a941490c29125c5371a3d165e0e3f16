I"S<hr />

<h3 id="introduction">Introduction</h3>

<hr />

<p>비교적 최근에 크리스토퍼 놀란의 신작 영화인 테넷을 보았다. 그런데 여기서 어딘가 정감가는 용어가 눈에 들어왔는데 바로 <strong>엔트로피</strong>였다. 엔트로피 어디서 들어본 것 같았는데… 바로 매번 모델에서 loss 함수에 사용했었던 교차 엔트로피랑 비슷해서 였다.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># model.compile 에서 지긋지긋하게 쓰는 녀석들
</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="s">'Adam'</span><span class="p">,</span>
<span class="n">loss</span> <span class="o">=</span> <span class="s">'categorical_crossentropy'</span><span class="p">,</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s">'accuracy'</span><span class="p">]</span>
<span class="p">)</span></code></pre></figure>

<p>그래서 인지 문득 궁금해졌다. 내가 항상 사용하는 loss 함수에 대한 개념을 잘 알고 있는가? 음.. 아닌 것 같다,, 그래서 이번 포스팅은 엔트로피란 무엇이고 교차 엔트로피란 또 무엇인지를 공부하는 겸 다뤄보려고 한다.</p>

<hr />

<h3 id="entropy">Entropy</h3>

<hr />

<p>엔트로피(Entropy)란 정보이론 뿐 아니라 열역학에서도 나오는 용어인데, 정보이론에서의 엔트로피를 이야기 해보자. 정보이론에서 높은 확률은 정보량이 작음을 의미한다(정보량은 사건의 확률과 관련이 있다). 예를 들면 한 여름에 맑을 확률은 높다 하지만 갑자기 눈이 내릴 확률은 거의 희박할 것이다. 그렇다면 핵심은 높은 확률에서는 정보량이 작고 낮은 확률에서는 정보량이 그만큼 크다는 것이다. 쉽게, <strong>맑음 = 정보량 작음, 눈 = 정보량 큼</strong> 이렇게 생각하면 편할 것 같다. 다른 말로 놀람의 정도라고도 표현할 수 있다(한 여름에 눈이 왔으니 놀랄만 하지 않은가).
<br />
<br /></p>

<p>정보량을 나타내는 함수를 <strong>h</strong>라고 할 때,</p>

<ul>
  <li>확률 변수 <strong>X</strong> 가 존재하고 X는 sunny(맑음), Snow(눈)이라는 두 가지 값을 가질 수 있다.</li>
  <li><strong>X의 정보량 h(x)는 p(x)에 대한 함수 즉 h = f(p)이다.</strong> (p는 x가 벌어질 확률)</li>
  <li><strong>p(sunny) = 0.85, p(snow) = 0.15</strong> (물론 눈이 올 확률은 0.15보다 훨 낮겠지만..)</li>
  <li><strong>h(snow) &gt; h(sunny)</strong> 이어야 한다. 정보량과 확률은 반비례하니까 당연하다.
<br />
<br />
추가적으로 독립 사건 x1, x2가 존재할 때 아래와 같은 수식이 적용된다.</li>
</ul>

<p>[h(x1, x2) = h(x1) + h(x2)]</p>

<p>[p(x1, x2) = p(x1) \times p(x2)]</p>

<p><br /></p>

<p>결론적으로 정보량에 대한 식이 아래와 같이 도출된다.</p>

<p>[\ h(x) = -log_{2}p(x)]</p>

<p>여기서 log의 밑이 2인 이유는 정보량의 단위를 비트 수(0, 1)로 나타낼 수 있어서이다.
그렇다면 평균적인 정보량은 아래와 같다.</p>

<p>[h(snow) \times p(snow) + h(sunny) \times p(sunny)]</p>

<p><br />
그리하여 아래와 같은 수식이 완성되고 바로 이 값이 <strong>엔트로피</strong>이다.</p>

<p>[\ H(X) = -\sum\limits_{i=1}^np(x_i)logp(x_i)]</p>

<p>엔트로피는 확률이 한쪽으로 몰릴 때보다 각각의 확률이 균등할 때 엔트로피 값이 가장 커지게 되는데 이로서 불확실성의 정도를 파악 가능하게 된다.</p>

<hr />

<h3 id="cross-entropy">Cross Entropy</h3>

<hr />

<p>엔트로피는 하나의 확률 분포에 대한 측정하기 위한 지표였으나, 교차 엔트로피는 분포 p, q 사이의 차이를 측정하기 위한 지표이다. 여기서 p를 label(정답) 값, q를 예측값(잘못된 정보) 즉 머신러닝이나 딥러닝에서의 output 값으로 생각하면 된다.</p>

<p>[\ H(p, q) = -\sum\limits_{i=1}^np(x_i)logq(x_i)]</p>

<p>이전의 엔트로피 공식과의 차이점이 뭘까? 바로 log 값으로 \(p(x)\)가 아닌 \(q(x)\)가 온 것이다. 간단히 설명하자면 q라는 잘못된 정보로 얻은 엔트로피 값이다.</p>

<p>여기서 오차가 클수록 교차 엔트로피 값은 점점 커지게 된다. 즉 교차 엔트로피를 loss 함수로 쓰게 되면 이 교차 엔트로피 값을 최소화시키는 게 모델의 궁극적인 목표가 될 것이다.</p>

<p><br />
<br /></p>

<hr />

<p><br /></p>

<p>이 글은 팡요랩의 <a href="https://youtu.be/Dc0PQlNQhGY">KL Divergence 강의 영상</a> 을 참고하여 작성하였습니다.</p>

<p>[]</p>

<p>[]</p>

<p>[]</p>

<p>[]</p>

<p>[]</p>
:ET